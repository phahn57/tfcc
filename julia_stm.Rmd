---
title: "julia_stm"
author: "Peter Hahn"
date: "11 11 2018"
output: html_document
---
#Links
[https://github.com/dondealban/learning-stm#workflow]


```{r setup, include=FALSE, message=FALSE}
knitr::opts_chunk$set(echo = FALSE, include = TRUE, message=FALSE, fig.width = 12, fig.height = 8)
## load libraries
library(tidyverse)
#library(RISmed)
library(wordcloud)
library(stringr)
library(igraph)
#library(ggraph)
#library(tidygraph)
library(tm)
library(tidytext)
#library(widyr)
library(topicmodels)
library(furrr)
library(stm)
library(reshape2)
library(funModeling)
```
# Loading data alternatively laod prepared files
For this analysis a pubmed query was done with search-term: "Dupuytren Contracture[Mesh]". That reveals 2286 documents. 
From the query this fields were extracted: title, abstract, journal, DOI, year
```{r}
 # load prefabricated csv build by first part of preprocess.Rmd
        pm_data <- read_csv("abstracts.csv")

# make bins for publication year
        pm_data$year_cut <- cut(pm_data$year, c(1960, 1970, 1980, 1990, 2000,2010,2018),labels=c("60-69","70-79","80-89","90-99","00-09","10-18"), include.lowest=TRUE)
        
        
## remove plural words and digits, then 
       pm_data <-pm_data  %>% mutate(abstract=str_replace_all(abstract,"options","option")) %>% filter(abstract!="") %>% mutate(abstract=str_replace_all(abstract,"(\\d)","")) %>%
               mutate(abstract=str_replace_all(abstract,"\\."," "))
# list of words to remove in token
        del_word <-  tibble(word=c("dupuytren","NA","patient","disease","treatment","study","result","hand","disease",
                               "contracture","patients","dupuytren's","clinical","results","report","included","month","treated","found","increased","compared","ci","statistically","lt","studies","underwent","diagnosis","reported","de","quot","des","bev","major","dd","background","purpose","objective","iii","ii"))
        

```
Documents without abstract are removed, thus `r nrow(pm_data)` documents are remaining.

# Number of papers per year: self- explaining
```{r include=TRUE}
        pm_data %>%
          group_by(year) %>%
          count() %>%
          #filter(year > 1960) %>%
          ggplot(aes(year, n)) +
          geom_point() +
          geom_line() +
          labs(title = "Pubmed articles with search term `Dupuytren` \n1960-2018", hjust = 0.5, y = "Articles")+ theme_bw()
```          

## basic NLP( natural language processing)
unnest tokens, remove stop words and remove frequently common words:
"dupuytren","NA","patient","disease","treatment","study","result","hand","disease","contracture","patients","dupuytren's","clinical","results","report","included","month","treated","found","increased","compared","95","ci","10","20","30","statistically","lt","studies","underwent","diagnosis","12","15","50","reported","de","quot","des","bev","major","dd","background","purpose","objective"
```{r}

        pm_abstract <- pm_data %>% 
                unnest_tokens(word,abstract) %>% 
                anti_join(stop_words)
        ## some special filters
        pm_abstract <- pm_abstract %>% 
                anti_join(del_word) 
```

```{r}
        word_counts <- pm_abstract %>% 
        count(DOI,word,sort=TRUE) %>% ungroup()
```

## Inspect frequent words

```{r}
        freq_words <- word_counts %>% count(word) %>% arrange(desc(nn))
```


## build document term matrix for further analysis two ways
```{r}
        abstr_dtm <- word_counts %>% 
        cast_dtm(DOI,word,n)

        abstr_sparse <- word_counts %>% 
                cast_sparse(DOI,word,n)
```

### use lda_tuning
```{r}

                system.time({
  tunes <- FindTopicsNumber(
    dtm = abstr_dtm,
    topics = seq(from = 20, to = 100, by = 10),
    metrics = c("Griffiths2004", "CaoJuan2009", "Arun2010"),
    method = "Gibbs",
    control = list(seed = 12345),
    mc.cores = 4L,
    verbose = TRUE
  )
})

```

```{r}
FindTopicsNumber_plot(tunes)
```



## final model with k = 50

```{r}
topic_model <- stm(abstr_sparse, K = 30, 
                   verbose = FALSE, init.type = "Spectral")


```
 
## alternative with LD
```{r}
        abstr_lda <- LDA(abstr_dtm,k=50,control=list(seed=1234))
```


```{r}
        ##save if necessary or load
        # save(topic_model,file="topic_60.RData")
        tidy_lda <- tidy(abstr_lda)
```
## top terms

```{r}
        top_terms <- tidy_lda %>% 
        group_by(topic) %>%
        filter(!is.na(term)) %>% 
        top_n(10,beta) %>% 
        ungroup() %>% 
        arrange(topic,-beta)
        
```

### Graphical representation of top terms

For each topic the top 10 terms representing the topic are presented.
```{r include=TRUE}
        top_terms %>% 
  mutate(term = reorder(term, beta)) %>%
  group_by(topic, term) %>%    
  arrange(desc(beta)) %>%  
  ungroup() %>%
  mutate(term = factor(paste(term, topic, sep = "__"), 
                       levels = rev(paste(term, topic, sep = "__")))) %>%
  ggplot(aes(term, beta, fill = as.factor(topic))) +
  geom_col(show.legend = FALSE) +
  coord_flip() +
  scale_x_discrete(labels = function(x) gsub("__.+$", "", x)) +
  labs(title = "Top 10 terms in each LDA topic",
       x = NULL, y = expression(beta)) +
  facet_wrap(~ topic, ncol = 6, scales = "free")

```

# Vice versa representation
We just explored which words are associated with which topics. Next, let’s examine which topics are associated with which description fields (i.e., documents). We will look at a different probability for this, γ, the probability that each document belongs in each topic.

Extract gamma , arrange by document and gamma and extract top 5 gamma for each document 

```{r}
lda_gamma <- tidy(abstr_lda,matrix="gamma") %>% arrange(document,desc(gamma)) %>% group_by(document) %>% top_n(5,gamma)
```

## How are the probabilities distributed? 
First notice that the y-axis is plotted on a log scale; otherwise it is difficult to make out any detail in the plot. Next, notice that γ
  runs from 0 to 1; remember that this is the probability that a given document belongs in a given topic. There are many values near zero, which means there are many documents that do not belong in each topic. Also, there are many values near γ=1
 ; these are the documents that do belong in those topics. This distribution shows that documents are being well discriminated as belonging to a topic or not. 

```{r}
        ggplot(lda_gamma, aes(gamma)) +
  geom_histogram() +
  scale_y_log10() +
  labs(title = "Distribution of probabilities for all topics",
       y = "Number of documents", x = expression(gamma))
```

We can also look at how the probabilities are distributed within each topic.
```{r}
        ggplot(lda_gamma_50, aes(gamma, fill = as.factor(topic))) +
  geom_histogram(show.legend = FALSE) +
  facet_wrap(~ topic, ncol = 4) +
  scale_y_log10() +
  labs(title = "Distribution of probability for each topic",
       y = "Number of documents", x = expression(gamma))
```


Extract gamma , arrange by topic and gamma and extract top 10 gamma for each topic 

```{r}
lda_gamma_topic <- tidy(abstr_lda,matrix="gamma") %>% arrange(topic,desc(gamma)) %>% group_by(topic) %>% top_n(20,gamma)
lda_gamma_topic$document <- as.numeric(lda_gamma_topic$document)
```

## join with pm_data to reveal title etc.

```{r}
 topic_joined <- left_join(lda_gamma_topic,pm_data, by=c("document"="DOI"))

```






